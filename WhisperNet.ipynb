{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Imports\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import glob\n",
    "import itertools\n",
    "import numpy as np\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from pathlib import Path\n",
    "from tensorflow.keras import applications\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras import metrics\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.applications import resnet\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "tf.executing_eagerly()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word and Phrases Extraction"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "words = set()\n",
    "people = set()\n",
    "for name in glob.glob(\"crema_cropped/*.gif\"):\n",
    "    name = name.replace(\"crema_cropped/\",\"\")\n",
    "    s = name.split(\"_\")\n",
    "    people.add(s[0])\n",
    "    words.add(s[1])\n",
    "words = list(words)\n",
    "people = list(people)\n",
    "len(people)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Triplet Generation"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "np.random.shuffle(people)\n",
    "people_train, people_val, people_test = people[:66], people[66:77], people[77:]\n",
    "\n",
    "def get_triplets(ppl):\n",
    "  c = 0\n",
    "  triplets = []\n",
    "  for person in ppl:\n",
    "    for word in words:\n",
    "      for i in range(1,7):\n",
    "        anchor = \"crema_cropped/\" + person + \"_\" + word + \"_\" + str(i) + '.gif'\n",
    "        positives = [\"crema_cropped/\" + x[0] + x[1] + '.gif' for x in list(itertools.product([person + \"_\" + word + \"_\"],[str(j) for j in range(1,7) if i != j]))]\n",
    "        negatives = [\"crema_cropped/\" + x[0] + x[1] + x[2] + '.gif' for x in list(itertools.product([person + \"_\"],[w + \"_\" for w in list(words) if w != word],[str(j) for j in range(1,7)])) + list(itertools.product([p + \"_\" for p in ppl if p != person],[word+ \"_\"],[str(j) for j in range(1,7)]))]\n",
    "        prod = list(itertools.product([anchor],positives,negatives))\n",
    "        triplets += prod\n",
    "  np.random.shuffle(triplets)\n",
    "  return triplets\n",
    "\n",
    "\n",
    "triplets_train = get_triplets(people_train)\n",
    "triplets_val = get_triplets(people_val)\n",
    "triplets_test = get_triplets(people_test)\n",
    "(len(triplets_train),len(triplets_val),len(triplets_test))"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(10834560, 498960, 498960)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Image and Landmark Pre-Processing"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "def preprocess_image(filename):\n",
    "    return tf.RaggedTensor.from_tensor(tf.expand_dims(tf.image.convert_image_dtype(tf.io.decode_gif(tf.io.read_file(filename)), tf.float32),0))\n",
    "\n",
    "def preprocess_images(anchor, positive, negative):\n",
    "  return (\n",
    "      preprocess_image(anchor),\n",
    "      preprocess_image(positive),\n",
    "      preprocess_image(negative),\n",
    "  )\n",
    "\n",
    "\n",
    "def preprocess_landmark(filename):\n",
    "    filename = tf.strings.regex_replace(filename, '\\.gif', '.tf')\n",
    "    data = tf.io.read_file(filename)\n",
    "    landmarks = tf.io.parse_tensor(data, out_type=tf.float32)\n",
    "    tensor = tf.convert_to_tensor(landmarks, dtype=tf.float32)\n",
    "    tensor.set_shape([None, 24, 2])\n",
    "    return tf.RaggedTensor.from_tensor(tf.expand_dims(tensor,0))\n",
    "\n",
    "def preprocess_landmarks(anchor, positive, negative):\n",
    "    return [\n",
    "        preprocess_landmark(anchor),\n",
    "        preprocess_landmark(positive),\n",
    "        preprocess_landmark(negative),\n",
    "    ]\n",
    "\n",
    "def squeeze(a,p,n):\n",
    "  return (\n",
    "      tf.squeeze(a, axis=1),\n",
    "      tf.squeeze(p, axis=1),\n",
    "      tf.squeeze(n, axis=1),\n",
    "  )\n",
    "\n",
    "def reshape(i,l):\n",
    "    return (\n",
    "        (i[0],l[0]),\n",
    "        (i[1],l[1]),\n",
    "        (i[2],l[2]) \n",
    "    )"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Creating train, validation and test datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "# Train\n",
    "dataset_train = tf.data.Dataset.zip((\n",
    "    tf.data.Dataset.from_tensor_slices(np.array(triplets_train)[:,0]),\n",
    "    tf.data.Dataset.from_tensor_slices(np.array(triplets_train)[:,1]),\n",
    "    tf.data.Dataset.from_tensor_slices(np.array(triplets_train)[:,2])\n",
    "))\n",
    "\n",
    "dataset_train = dataset_train.shuffle(buffer_size=1024)\n",
    "dataset_train_images = dataset_train.map(preprocess_images)\n",
    "dataset_train_landmarks = dataset_train.map(preprocess_landmarks)\n",
    "dataset_train_images = dataset_train_images.batch(32, drop_remainder=False)\n",
    "dataset_train_landmarks = dataset_train_landmarks.batch(32, drop_remainder=False)\n",
    "dataset_train_images = dataset_train_images.map(squeeze)\n",
    "dataset_train_landmarks = dataset_train_landmarks.map(squeeze)\n",
    "dataset_train = tf.data.Dataset.zip((dataset_train_images,dataset_train_landmarks))\n",
    "dataset_train = dataset_train.map(reshape)\n",
    "dataset_train = dataset_train.prefetch(32)\n",
    "\n",
    "\n",
    "# Val\n",
    "dataset_val = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(np.array(triplets_val)[:,0]),\n",
    "                                     tf.data.Dataset.from_tensor_slices(np.array(triplets_val)[:,1]),\n",
    "                                     tf.data.Dataset.from_tensor_slices(np.array(triplets_val)[:,2])))\n",
    "dataset_val = dataset_val.shuffle(buffer_size=1024)\n",
    "dataset_val_images = dataset_val.map(preprocess_images)\n",
    "dataset_val_landmarks = dataset_val.map(preprocess_landmarks)\n",
    "dataset_val_images = dataset_val_images.batch(32, drop_remainder=False)\n",
    "dataset_val_landmarks = dataset_val_landmarks.batch(32, drop_remainder=False)\n",
    "dataset_val_images = dataset_val_images.map(squeeze)\n",
    "dataset_val_landmarks = dataset_val_landmarks.map(squeeze)\n",
    "dataset_val = tf.data.Dataset.zip((dataset_val_images,dataset_val_landmarks))\n",
    "dataset_val = dataset_val.map(reshape)\n",
    "dataset_val = dataset_val.prefetch(32)\n",
    "\n",
    "# Test\n",
    "dataset_test = tf.data.Dataset.zip((tf.data.Dataset.from_tensor_slices(np.array(triplets_test)[:,0]),\n",
    "                                     tf.data.Dataset.from_tensor_slices(np.array(triplets_test)[:,1]),\n",
    "                                     tf.data.Dataset.from_tensor_slices(np.array(triplets_test)[:,2])))\n",
    "dataset_test = dataset_test.shuffle(buffer_size=1024)\n",
    "dataset_test_images = dataset_test.map(preprocess_images)\n",
    "dataset_test_landmarks = dataset_test.map(preprocess_landmarks)\n",
    "dataset_test_images = dataset_test_images.batch(32, drop_remainder=False)\n",
    "dataset_test_landmarks = dataset_test_landmarks.batch(32, drop_remainder=False)\n",
    "dataset_test_images = dataset_test_images.map(squeeze)\n",
    "dataset_test_landmarks = dataset_test_landmarks.map(squeeze)\n",
    "dataset_test = tf.data.Dataset.zip((dataset_test_images,dataset_test_landmarks))\n",
    "dataset_test = dataset_test.map(reshape)\n",
    "dataset_test = dataset_test.prefetch(32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# LipNet\n",
    "input_images = layers.Input((40, 18, 30, 3))\n",
    "input_landmarks = layers.Input((40, 24, 2))\n",
    "\n",
    "zero1 = layers.ZeroPadding3D(padding=(1, 2, 2), name='zero1')(input_images)\n",
    "conv1 = layers.Conv3D(32, (3, 5, 5), strides=(1, 2, 2), padding='same', activation='relu', kernel_initializer='he_normal', name='conv1')(input_images)\n",
    "maxp1 = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max1')(conv1)\n",
    "drop1 = layers.Dropout(0.5)(maxp1)\n",
    "\n",
    "zero2 = layers.ZeroPadding3D(padding=(1, 2, 2), name='zero2')(drop1)\n",
    "conv2 = layers.Conv3D(64, (3, 5, 5), strides=(1, 1, 1), activation='relu', kernel_initializer='he_normal', name='conv2')(zero2)\n",
    "maxp2 = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max2')(conv2)\n",
    "drop2 = layers.Dropout(0.5)(maxp2)\n",
    "\n",
    "zero3 = layers.ZeroPadding3D(padding=(1, 1, 1), name='zero3')(drop2)\n",
    "conv3 = layers.Conv3D(96, (3, 3, 3), strides=(1, 1, 1), activation='relu', kernel_initializer='he_normal', name='conv3')(zero3)\n",
    "maxp3 = layers.MaxPooling3D(pool_size=(1, 2, 2), strides=(1, 2, 2), name='max3')(conv3)\n",
    "drop3 = layers.Dropout(0.5)(maxp3)\n",
    "\n",
    "resh1 = layers.TimeDistributed(layers.Flatten())(drop3)\n",
    "\n",
    "gru_1 = layers.Bidirectional(layers.GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru1'), merge_mode='concat')(resh1)\n",
    "gru_2 = layers.Bidirectional(layers.GRU(256, return_sequences=True, kernel_initializer='Orthogonal', name='gru2'), merge_mode='concat')(gru_1)\n",
    "flatten = layers.Flatten()(gru_2)\n",
    "\n",
    "td1 = layers.TimeDistributed(layers.Dense(4),name=\"td1\")(input_landmarks)\n",
    "td2 = layers.TimeDistributed(layers.Dense(8))(td1)\n",
    "td3 = layers.TimeDistributed(layers.Dense(16))(td2)\n",
    "td4 = layers.TimeDistributed(layers.Flatten(),name=\"t2\")(td3)\n",
    "l_lstm = layers.LSTM(64,return_sequences=False)(td4)\n",
    "concat = tf.keras.layers.Concatenate(axis=1)([flatten,l_lstm])\n",
    "\n",
    "b4_l2 = layers.BatchNormalization()(concat)\n",
    "b4_l3 = layers.Dropout(.1)(b4_l2)\n",
    "b4_l4 = layers.Dense(512)(b4_l3)\n",
    "b4_l5 = layers.Dense(256)(b4_l4)\n",
    "\n",
    "embedding = Model(inputs=(input_images, input_landmarks), outputs=b4_l5)\n",
    "embedding.summary()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 40, 18, 30,  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1 (Conv3D)                  (None, 40, 9, 15, 32 7232        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max1 (MaxPooling3D)             (None, 40, 4, 7, 32) 0           conv1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 40, 4, 7, 32) 0           max1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero2 (ZeroPadding3D)           (None, 42, 8, 11, 32 0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2 (Conv3D)                  (None, 40, 4, 7, 64) 153664      zero2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max2 (MaxPooling3D)             (None, 40, 2, 3, 64) 0           conv2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 40, 2, 3, 64) 0           max2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zero3 (ZeroPadding3D)           (None, 42, 4, 5, 64) 0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv3 (Conv3D)                  (None, 40, 2, 3, 96) 165984      zero3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "max3 (MaxPooling3D)             (None, 40, 1, 1, 96) 0           conv3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 40, 24, 2)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 40, 1, 1, 96) 0           max3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "td1 (TimeDistributed)           (None, 40, 24, 4)    12          input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 40, 96)       0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 40, 24, 8)    40          td1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional (Bidirectional)   (None, 40, 512)      543744      time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 40, 24, 16)   144         time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 40, 512)      1182720     bidirectional[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "t2 (TimeDistributed)            (None, 40, 384)      0           time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 20480)        0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     (None, 64)           114944      t2[0][0]                         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 20544)        0           flatten_1[0][0]                  \n",
      "                                                                 lstm[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 20544)        82176       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 20544)        0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 512)          10519040    dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 256)          131328      dense_3[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 12,901,028\n",
      "Trainable params: 12,859,940\n",
      "Non-trainable params: 41,088\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Plot of the Network Architecture : <br>\n",
    "<img src=\"https://github.com/ab2llah/WhisperNet/raw/main/whisper-lipnet.png\">"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Siamese Network"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "class DistanceLayer(layers.Layer):\n",
    "    \"\"\"\n",
    "    This layer is responsible for computing the distance between the anchor\n",
    "    embedding and the positive embedding, and the anchor embedding and the\n",
    "    negative embedding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, anchor, positive, negative):\n",
    "        ap_distance = tf.reduce_sum(tf.square(anchor - positive), -1)\n",
    "        an_distance = tf.reduce_sum(tf.square(anchor - negative), -1)\n",
    "        #print(ap_distance, an_distance)\n",
    "        return (ap_distance, an_distance)\n",
    "\n",
    "anchor_input_image = layers.Input(name=\"anchor_image\", shape=(None, 18, 30, 3))\n",
    "positive_input_image = layers.Input(name=\"positive_image\", shape=(None, 18, 30, 3))\n",
    "negative_input_image = layers.Input(name=\"negative_image\", shape=(None, 18, 30, 3))\n",
    "\n",
    "anchor_input_landmark = layers.Input(name=\"anchor_landmark\", shape=(None, 24, 2))\n",
    "positive_input_landmark = layers.Input(name=\"positive_landmark\", shape=(None, 24, 2))\n",
    "negative_input_landmark = layers.Input(name=\"negative_landmark\", shape=(None, 24, 2))\n",
    "\n",
    "\n",
    "distances = DistanceLayer()(\n",
    "    embedding((anchor_input_image, anchor_input_landmark)),\n",
    "    embedding((positive_input_image, positive_input_landmark)),\n",
    "    embedding((negative_input_image, negative_input_landmark)),\n",
    ")\n",
    "\n",
    "siamese_network = Model(\n",
    "    inputs=[(anchor_input_image, anchor_input_landmark),\n",
    "            (positive_input_image, positive_input_landmark),\n",
    "            (negative_input_image, negative_input_landmark)],\n",
    "    outputs=distances\n",
    ")\n",
    "\n",
    "\n",
    "class SiameseModel(Model):\n",
    "    \"\"\"The Siamese Network model with a custom training and testing loops.\n",
    "\n",
    "    Computes the triplet loss using the three embeddings produced by the\n",
    "    Siamese Network.\n",
    "\n",
    "    The triplet loss is defined as:\n",
    "       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)\n",
    "       d(a,p) <= d(a,n) => d(a,p) - d(a,n) + margin <= 0 \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    def __init__(self, siamese_network, margin=0.5):\n",
    "        super(SiameseModel, self).__init__()\n",
    "        self.siamese_network = siamese_network\n",
    "        self.margin = margin\n",
    "        self.loss_tracker = metrics.Mean(name=\"loss\")\n",
    "        self.accuracy_tracker = metrics.Mean(name=\"accuracy\")\n",
    "        self.val = 0\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \n",
    "        return self.siamese_network([inputs[\"images\"],inputs[\"landmarks\"]])\n",
    "\n",
    "    def train_step(self, data):\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            loss = self._compute_loss(data)\n",
    "\n",
    "        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)\n",
    "\n",
    "        self.optimizer.apply_gradients(\n",
    "            zip(gradients, self.siamese_network.trainable_weights)\n",
    "        )\n",
    "        accuracy = self._compute_accuracy(data)\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.accuracy_tracker.update_state(accuracy)\n",
    "        return {\"loss\": self.loss_tracker.result(),  \"accuracy\": self.accuracy_tracker.result()}\n",
    "\n",
    "    def test_step(self, data):\n",
    "        loss = self._compute_loss(data)\n",
    "        accuracy = self._compute_accuracy(data)\n",
    "\n",
    "        self.loss_tracker.update_state(loss)\n",
    "        self.accuracy_tracker.update_state(accuracy)\n",
    "        return {\"loss\": self.loss_tracker.result(), \"accuracy\": self.accuracy_tracker.result()}\n",
    "\n",
    "    def _compute_loss(self, data):\n",
    "        ap_distance, an_distance = self.siamese_network(data)\n",
    "        loss = ap_distance - an_distance\n",
    "        self.val = loss\n",
    "        loss = tf.maximum(loss + self.margin, 0.0)\n",
    "        return loss\n",
    "    \n",
    "    def _compute_accuracy(self, val):\n",
    "        return 1 / (1 + tf.exp(2 * self.val))\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [self.loss_tracker, self.accuracy_tracker]\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpoint = ModelCheckpoint(\"Models/crema_combined0.tf\", \n",
    "                             monitor = 'val_loss',\n",
    "                             mode = 'min',\n",
    "                             save_best_only = True,\n",
    "                             save_weights_only=True, \n",
    "                             verbose = 1)\n",
    "\n",
    "siamese_model = SiameseModel(siamese_network)\n",
    "siamese_model.compile(optimizer=optimizers.Adam(0.001))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training Processs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "NAME = \"crema_combined0\"\n",
    "tensorboard = TensorBoard(log_dir=f\"logs/{NAME}\")\n",
    "\n",
    "siamese_model.fit(\n",
    "    dataset_train,\n",
    "    epochs=500,\n",
    "    verbose='auto',\n",
    "    steps_per_epoch=16,\n",
    "    callbacks=[checkpoint,tensorboard],\n",
    "    validation_data=dataset_val,\n",
    "    shuffle=False,\n",
    "    use_multiprocessing=True,\n",
    "    validation_steps=4\n",
    ")\n",
    "\n"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.5 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}